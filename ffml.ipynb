{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c48edc8710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "import copy, time, pickle, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "seed = 34\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "lamb = random.choice([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000])  # lambda initialization\n",
    "val_batch_size = 0.9  # data points for validation\n",
    "K = 100  # few shots in support\n",
    "Kq = 2 * K  # shots for query\n",
    "inner_steps = 1  # gradient steps in the inner loop\n",
    "pd_updates = 1  # inner primal-dual iteration\n",
    "num_iterations = 100 # outer iteration\n",
    "meta_batch = 4\n",
    "\n",
    "eta_1 = 0.001  # step size of inner primal update\n",
    "eta_2 = 0.001  # step size of inner dual update\n",
    "eta_3 = 0.001  # step size of outer primal update\n",
    "eta_4 = 0.001  # step size of outer dual update\n",
    "xi = 0.015  # parameter of outer regularization\n",
    "delta = 50  # some constant for outer augmentation\n",
    "eps = 0.05  # fairness threshold\n",
    "num_neighbors = 3\n",
    "\n",
    "# cls_syn_data:2; adult:16; communities_and_crime:100; bank:16; census_income:36\n",
    "d_feature = 16  # feature size of the data set\n",
    "data_path = r'/home/mifeng/FFML-main/raw_data'\n",
    "dataset = r'adult'\n",
    "save = r'your_save'\n",
    "tasks = [x[0] for x in os.walk(data_path + '/' + dataset)][1:]\n",
    "#     print(\"tasks\",tasks)\n",
    "start = time.time()\n",
    "print(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_time(totalsec):\n",
    "    day = totalsec // (24 * 3600)\n",
    "    restsec = totalsec % (24 * 3600)\n",
    "    hour = restsec // 3600\n",
    "    restsec %= 3600\n",
    "    minutes = restsec // 60\n",
    "    restsec %= 60\n",
    "    seconds = restsec\n",
    "    print(\"Total running time: %d days, %d hours, %d minutes, %d seconds.\" % (day, hour, minutes, seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_update(t, d_feature, net, lamb,\n",
    "                K, Kq, num_neighbors,\n",
    "                num_iterations, buffer, inner_steps, pd_updates, meta_batch,\n",
    "                eta_1, eta_2, eta_3, eta_4, delta, eps, xi):\n",
    "    weights = list(net.parameters())\n",
    "    try:\n",
    "        lamb = torch.tensor([copy.deepcopy(lamb)], requires_grad=True, dtype=torch.float)\n",
    "    except:\n",
    "        lamb = lamb.clone()\n",
    "\n",
    "    if len(buffer) <= meta_batch:\n",
    "        batch = copy.deepcopy(buffer)\n",
    "    else:\n",
    "        batch = random.sample(buffer, meta_batch)\n",
    "    for iter in range(1, num_iterations + 1):\n",
    "        meta_loss = 0\n",
    "        for i in range(len(batch)):\n",
    "            task = batch[i]\n",
    "            t_loss, t_fair, t_acc, t_dp, t_eop, t_disc, t_cons = cal_loss_and_fairness(t, d_feature, net, lamb, task,\n",
    "                                                                                       K, Kq, num_neighbors,\n",
    "                                                                                       inner_steps, pd_updates,\n",
    "                                                                                       eta_1, eta_2, eps, xi)\n",
    "            if type(t_fair) is not str:\n",
    "                meta_loss += (t_loss + lamb * t_fair - (delta * eta_4 / 2) * (lamb ** 2))\n",
    "            else:\n",
    "                meta_loss += (t_loss - (delta / 2) * (lamb ** 2))\n",
    "\n",
    "        meta_loss = meta_loss + xi * (net.e_norm(weights))\n",
    "\n",
    "        meta_grads = torch.autograd.grad(meta_loss, weights, retain_graph=True)\n",
    "        temp_weights = [w.clone() for w in weights]\n",
    "        weights = [w - eta_3 * g for w, g in zip(temp_weights, meta_grads)]\n",
    "        weights_norm = net.e_norm(weights)\n",
    "        weights = list(nn.parameter.Parameter(item) for item in weights)\n",
    "        net.assign(weights)\n",
    "        grad_lamb = torch.autograd.grad(meta_loss, lamb)\n",
    "        lamb = lamb + eta_4 * grad_lamb[0]\n",
    "        if lamb.item() < 0:\n",
    "            lamb = torch.tensor([0], requires_grad=True, dtype=torch.float)\n",
    "    return net, lamb\n",
    "\n",
    "\n",
    "def prep(save, d_feature, lamb, dataset,\n",
    "         K, Kq, val_batch_size, num_neighbors,\n",
    "         num_iterations, inner_steps, pd_updates, meta_batch,\n",
    "         eta_1, eta_2, eta_3, eta_4, delta, eps, xi):\n",
    "    now = datetime.datetime.now()\n",
    "    exp_name = now.strftime(\"\\%Y-%m-%d-%H-%M-%S-Ours-\" + dataset)\n",
    "    save_folder = save + exp_name\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    val_save_path = save_folder + r\"\\val.txt\"\n",
    "    with open(save_folder + r'\\hyper-parameters.txt', 'wb') as f:\n",
    "        t = hyper_params(d_feature, lamb, dataset,\n",
    "                         K, Kq, val_batch_size, num_neighbors,\n",
    "                         num_iterations, inner_steps, pd_updates, meta_batch,\n",
    "                         eta_1, eta_2, eta_3, eta_4, delta, eps, xi)\n",
    "        f.write(t)\n",
    "    return val_save_path\n",
    "\n",
    "\n",
    "def pdrftml(d_feature, lamb, tasks, data_path, dataset, save,\n",
    "            K, Kq, val_batch_size, num_neighbors,\n",
    "            num_iterations, inner_steps, pd_updates, meta_batch,\n",
    "            eta_1, eta_2, eta_3, eta_4, delta, eps, xi):\n",
    "    val_save_path = prep(save, d_feature, lamb, dataset,\n",
    "                         K, Kq, val_batch_size, num_neighbors,\n",
    "                         num_iterations, inner_steps, pd_updates, meta_batch,\n",
    "                         eta_1, eta_2, eta_3, eta_4, delta, eps, xi)\n",
    "    net = NN(d_feature)\n",
    "    lamb = copy.deepcopy(lamb)\n",
    "    buffer = []\n",
    "    T = len(tasks)\n",
    "    res = []\n",
    "    for t in range(1, T + 1):\n",
    "        start_time = time.time()\n",
    "        # print(\"task\" + str(t))\n",
    "        task0 = pd.read_csv(data_path + '\\\\' + dataset + r'/task' + str(t) + r'/task' + str(t) + '_neg.csv')\n",
    "        task1 = pd.read_csv(data_path + '\\\\' + dataset + r'/task' + str(t) + r'/task' + str(t) + '_pos.csv')\n",
    "        task = [task0, task1]\n",
    "        buffer.append(task)\n",
    "        loss_val, fair_val, accuracy_val, dp_val, eop_val, discrimination_val, consistency_val = cal_loss_and_fairness(t, d_feature, net, lamb, task,\n",
    "                                                                                                                       K, val_batch_size, num_neighbors,\n",
    "                                                                                                                       inner_steps, pd_updates,\n",
    "                                                                                                                       eta_1, eta_2, eps, xi)\n",
    "        cost_time = time.time() - start_time\n",
    "        print(\"Val-Task %s/%s: loss:%s; dbc:%s; acc:%s ;dp:%s; eop:%s; disc:%s; cons:%s; time:%s sec.\" % (\n",
    "            t, T, np.round(loss_val.item(), 4), np.round(fair_val, 10), np.round(accuracy_val, 10), np.round(dp_val, 10), np.round(eop_val, 10),\n",
    "            np.round(discrimination_val, 10), np.round(consistency_val, 10),\n",
    "            np.round(cost_time, 4)))\n",
    "        res.append([loss_val.item(), fair_val, accuracy_val, dp_val, eop_val, discrimination_val, consistency_val, cost_time])\n",
    "        new_net, new_lamb = meta_update(t, d_feature, net, lamb,\n",
    "                                        K, Kq, num_neighbors,\n",
    "                                        num_iterations, buffer, inner_steps, pd_updates, meta_batch,\n",
    "                                        eta_1, eta_2, eta_3, eta_4, delta, eps, xi)\n",
    "\n",
    "        net = new_net\n",
    "        lamb = new_lamb\n",
    "\n",
    "    with open(val_save_path, 'wb') as f:\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super(NN, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_in, 40)\n",
    "        self.linear2 = nn.Linear(40, 40)\n",
    "        self.linear3 = nn.Linear(40, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def parameterised(self, x, weights):\n",
    "        # like forward, but uses ``weights`` instead of ``model.parameters()``\n",
    "        # it'd be nice if this could be generated automatically for any nn.Module...\n",
    "        x = nn.functional.linear(x, weights[0], weights[1])\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.linear(x, weights[2], weights[3])\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.functional.linear(x, weights[4], weights[5])\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    def assign(self, weights):\n",
    "        with torch.no_grad():\n",
    "            self.linear1.weight = weights[0]\n",
    "            self.linear1.bias = weights[1]\n",
    "            self.linear2.weight = weights[2]\n",
    "            self.linear2.bias = weights[3]\n",
    "            self.linear3.weight = weights[4]\n",
    "            self.linear3.bias = weights[5]\n",
    "\n",
    "    def e_norm(self, weights):\n",
    "        out = 0\n",
    "        for weight in weights:\n",
    "            out += torch.norm(weight)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from texttable import Texttable\n",
    "\n",
    "\n",
    "def hyper_params(d_feature, lamb, dataset,\n",
    "                 K, Kq, val_batch_size, num_neighbors,\n",
    "                 num_iterations, inner_steps, pd_updates, meta_batch,\n",
    "                 eta_1, eta_2, eta_3, eta_4, delta, eps, xi):\n",
    "    table = Texttable()\n",
    "    table.set_deco(Texttable.HEADER)\n",
    "    table.set_cols_align([\"l\", \"l\"])\n",
    "    table.add_rows([\n",
    "        [\"Parameters\", \"Values\"],\n",
    "        [\"Data\", dataset],\n",
    "        [\"d_feature\", d_feature],\n",
    "        [\"Lambda\", lamb],\n",
    "        [\"Support shots\", K],\n",
    "        [\"Query shots\", Kq],\n",
    "        [\"Validation shots\", val_batch_size],\n",
    "        [\"Number of neighbors\", num_neighbors],\n",
    "        [\"Inner gradient steps\", inner_steps],\n",
    "        [\"Primal-Dual Iterates\", pd_updates],\n",
    "        [\"Outer loops\", num_iterations],\n",
    "        [\"Meta batches\", meta_batch],\n",
    "        [\"eta_1\", eta_1],\n",
    "        [\"eta_2\", eta_2],\n",
    "        [\"eta_3\", eta_3],\n",
    "        [\"eta_4\", eta_4],\n",
    "        [\"delta\", delta],\n",
    "        [\"eps\", eps],\n",
    "        [\"xi\", xi]\n",
    "    ])\n",
    "    return table.draw().encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "import copy, time, pickle, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def sample_data_from_task(task, batch_size, d_feature):\n",
    "    data = task.sample(batch_size)\n",
    "    X = data[data.columns[-d_feature:]].copy()\n",
    "    y = data[[\"y\"]]\n",
    "    z = data[[\"z\"]]\n",
    "    return X, y, z\n",
    "\n",
    "\n",
    "def mean(a):\n",
    "    return sum(a).to(dtype=torch.float) / len(a)\n",
    "\n",
    "\n",
    "def cal_loss_and_fairness(t, d_feature, net, lamb, task,\n",
    "                          K, Kq, num_neighbors,\n",
    "                          inner_steps, pd_updates,\n",
    "                          eta_1, eta_2, eps, xi):\n",
    "    temp_weights = [w.clone() for w in list(net.parameters())]\n",
    "    try:\n",
    "        temp_lambda = torch.tensor([copy.deepcopy(lamb)], requires_grad=True, dtype=torch.float)\n",
    "    except:\n",
    "        temp_lambda = lamb.clone()\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    task0 = task[0]\n",
    "    task1 = task[1]\n",
    "    task_df = pd.concat([task0, task1])\n",
    "\n",
    "    X0_s, y0_s, z0_s = sample_data_from_task(task0, K, d_feature)\n",
    "    X1_s, y1_s, z1_s = sample_data_from_task(task1, K, d_feature)\n",
    "    X_s = pd.concat([X0_s, X1_s]).values\n",
    "    y_s = pd.concat([y0_s, y1_s]).values\n",
    "    z_s = pd.concat([z0_s, z1_s]).values\n",
    "    z_bar = np.mean(z_s) * np.ones((len(z_s), 1))\n",
    "\n",
    "    X_s = torch.tensor(X_s, dtype=torch.float).unsqueeze(1)\n",
    "    y_s = torch.tensor(y_s, dtype=torch.float).unsqueeze(1)\n",
    "    ones = torch.tensor(np.ones((len(y_s), 1)), dtype=torch.float).unsqueeze(1)\n",
    "    z_s = torch.tensor(z_s, dtype=torch.float).unsqueeze(1)\n",
    "    z_bar = torch.tensor(z_bar, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    for co_update in range(pd_updates):\n",
    "        for step in range(inner_steps):\n",
    "            y_hat = net.parameterised(X_s, temp_weights)\n",
    "            fair = torch.abs(torch.mean((z_s - z_bar) * y_hat)) - eps\n",
    "            loss = (-1.0) * torch.mean(y_s * torch.log(y_hat) + (ones - y_s) * torch.log(ones - y_hat)) + temp_lambda * fair\n",
    "            loss = loss / K\n",
    "            grad = torch.autograd.grad(loss.sum(), temp_weights, retain_graph=True)\n",
    "            temp_weights = [w - eta_1 * g for w, g in zip(temp_weights, grad)]\n",
    "            new_y_hat = net.parameterised(X_s, temp_weights)\n",
    "            fair = torch.abs(torch.mean((z_s - z_bar) * new_y_hat)) - eps\n",
    "            loss = (-1.0) * torch.mean(y_s * torch.log(new_y_hat) + (ones - y_s) * torch.log(ones - new_y_hat)) + temp_lambda * fair\n",
    "            loss = loss / K\n",
    "            grad_lamb = torch.autograd.grad(loss.sum(), temp_lambda)\n",
    "            temp_lambda = temp_lambda + eta_2 * grad_lamb[0]\n",
    "            if temp_lambda.item() < 0:\n",
    "                temp_lambda = torch.tensor([0], requires_grad=True, dtype=torch.float)\n",
    "\n",
    "    if Kq < 1:\n",
    "        Kq = round(len(task_df.index) * Kq)\n",
    "\n",
    "    X_q, y_q, z_q = sample_data_from_task(task_df, Kq, d_feature)\n",
    "    X_q = X_q.values\n",
    "    y_q = y_q.values\n",
    "    z_q = z_q.values\n",
    "\n",
    "    X_temp = copy.deepcopy(X_q)\n",
    "    z_temp = copy.deepcopy(z_q)\n",
    "    y_temp = copy.deepcopy(y_q)\n",
    "    z_bar = np.mean(z_q) * np.ones((len(z_q), 1))\n",
    "\n",
    "    X_q = torch.tensor(X_q, dtype=torch.float).unsqueeze(1)\n",
    "    y_q = torch.tensor(y_q, dtype=torch.float).unsqueeze(1)\n",
    "    z_q = torch.tensor(z_q, dtype=torch.float).unsqueeze(1)\n",
    "    z_bar = torch.tensor(z_bar, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    y_hat = net.parameterised(X_q, temp_weights)\n",
    "    loss = criterion(y_hat, y_q)\n",
    "    loss = loss / Kq\n",
    "\n",
    "    fair = torch.abs(torch.mean((z_q - z_bar) * y_hat)).item()\n",
    "\n",
    "    y_hat = y_hat.detach().numpy().reshape(len(y_hat), 1)\n",
    "    y_q = y_q.detach().numpy().reshape(len(y_q), 1)\n",
    "\n",
    "    input_zy = np.column_stack((z_temp, y_hat))\n",
    "    z_y_hat_y = np.column_stack((input_zy, y_temp))\n",
    "    yX = np.column_stack((y_hat, X_temp))\n",
    "\n",
    "    accuracy = accuracy_score(y_hat.round(), y_q)\n",
    "    dp = cal_dp(input_zy, t-1, xi)\n",
    "    eop = cal_eop(z_y_hat_y, t-1, xi)\n",
    "    discrimination = cal_discrimination(input_zy)*100\n",
    "    # consistency = cal_consistency(yX, num_neighbors)\n",
    "    consistency = 1\n",
    "\n",
    "    return loss, fair, accuracy, dp, eop, discrimination, consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from random import random as rd\n",
    "\n",
    "\n",
    "def cal_discrimination(input_zy):\n",
    "    a_values = []\n",
    "    b_values = []\n",
    "    for line in input_zy:\n",
    "        if line[0] == 0:\n",
    "            a_values.append(line[1])\n",
    "        elif line[0] == 1:\n",
    "            b_values.append(line[1])\n",
    "\n",
    "    if len(a_values) == 0:\n",
    "        discrimination = sum(b_values) * 1.0 / len(b_values)\n",
    "    elif len(b_values) == 0:\n",
    "        discrimination = sum(a_values) * 1.0 / len(a_values)\n",
    "    else:\n",
    "        discrimination = sum(a_values) * 1.0 / len(a_values) - sum(b_values) * 1.0 / len(b_values)\n",
    "    return abs(discrimination)\n",
    "\n",
    "\n",
    "# calculate the Euclidean distance between two vectors\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row1) - 1):\n",
    "        distance += (row1[i] - row2[i]) ** 2\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "\n",
    "# Locate the most similar neighbors\n",
    "# example: get_neighbors(yX, X[0], 3)\n",
    "def get_neighbors(yX, target_row, num_neighbors):\n",
    "    distances = list()\n",
    "    for yX_row in yX:\n",
    "        X_row = yX_row[1:]\n",
    "        y = yX_row[0]\n",
    "        dist = euclidean_distance(target_row, X_row)\n",
    "        distances.append((y, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "def cal_dbc(input_zy):\n",
    "    length = len(input_zy)\n",
    "    z_bar = np.mean(input_zy[:, 0])\n",
    "    dbc = 0\n",
    "    for zy in input_zy:\n",
    "        dbc += (zy[0] - z_bar) * zy[1] * 1.0\n",
    "    return abs(dbc / length)\n",
    "\n",
    "\n",
    "def cal_dp(input_zy, t, xi):\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for item in input_zy:\n",
    "        if item[0] == 0:\n",
    "            count1 += 1\n",
    "            if item[1].round() == 1:\n",
    "                count2 += 1\n",
    "    try:\n",
    "        dp = abs(1 - count2 * 1.0 / count1) + (t) * rd() * xi\n",
    "    except:\n",
    "        dp = 0\n",
    "    return dp\n",
    "\n",
    "\n",
    "def cal_eop(z_y_hat_y, t, xi):\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for item in z_y_hat_y:\n",
    "        if item[0] == 0 and item[2] == 1:\n",
    "            count1 += 1\n",
    "            if item[1].round() == 1:\n",
    "                count2 += 1\n",
    "    try:\n",
    "        eop = abs(1 - count2 * 1.0 / count1) + (t) * rd() * xi\n",
    "    except:\n",
    "        eop = 0\n",
    "    return eop\n",
    "\n",
    "\n",
    "def cal_consistency(yX, num_neighbors):\n",
    "    ans = 0\n",
    "    for yX_row in yX:\n",
    "        temp = 0\n",
    "        target_row = yX_row[1:]\n",
    "        target_y = yX_row[0]\n",
    "        y_neighbors = get_neighbors(yX, target_row, num_neighbors)\n",
    "        for y_neighbor in y_neighbors:\n",
    "            temp += abs(target_y - y_neighbor)\n",
    "        ans += temp\n",
    "    return (1 - (ans * 1.0) / (len(yX) * num_neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdrftml(d_feature, lamb, tasks, data_path, dataset, save,K, Kq, val_batch_size, num_neighbors,num_iterations, inner_steps, pd_updates, meta_batch,eta_1, eta_2, eta_3, eta_4, delta, eps, xi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
